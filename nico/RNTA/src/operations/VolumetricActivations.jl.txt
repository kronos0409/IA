# operations/VolumetricActivations.jl
# Implementa funciones de activación que operan en volúmenes completos

"""
    volumetric_activation(tensor; type=:adaptive_tanh, parameters=nothing)

Aplica una función de activación volumétrica al tensor 3D.
"""
function volumetric_activation(
    tensor::Array{T,3}; 
    type::Symbol=:adaptive_tanh, 
    parameters=nothing
) where T <: AbstractFloat
    # Determinar tipo de activación a aplicar
    if type == :adaptive_tanh
        return adaptive_tanh_activation(tensor, parameters)
    elseif type == :volswish
        return volumetric_swish(tensor, parameters)
    elseif type == :tensor_relu
        return tensor_relu(tensor, parameters)
    elseif type == :spatial
        return spatial_activation(tensor, parameters)
    elseif type == :contextual
        return contextual_activation(tensor, parameters)
    elseif type == :temporal
        return temporal_activation(tensor, parameters)
    elseif type == :feature
        return feature_activation(tensor, parameters)
    elseif type == :general
        # Activación general - combinación ponderada de diferentes tipos
        return general_activation(tensor, parameters)
    else
        # Tipo desconocido - usar tanh por defecto
        return tanh.(tensor)
    end
end

"""
    adaptive_tanh_activation(tensor, parameters=nothing)

Implementación de tanh con pendiente adaptativa basada en la magnitud local.
"""
function adaptive_tanh_activation(
    tensor::Array{T,3}, 
    parameters=nothing
) where T <: AbstractFloat
    # Parámetros por defecto si no se proporcionan
    slope_factor = isnothing(parameters) ? 0.1f0 : 
                   isa(parameters, ActivationParameters) ? parameters.slope_factor : 0.1f0
    
    # Aplicar tanh con pendiente adaptativa
    result = similar(tensor)
    
    for i in CartesianIndices(tensor)
        # La pendiente aumenta con la magnitud del valor
        adaptive_slope = 1.0f0 + slope_factor * abs(tensor[i])
        result[i] = tanh(tensor[i] * adaptive_slope)
    end
    
    return result
end

"""
    volumetric_swish(tensor, parameters=nothing)

Implementación volumétrica de Swish: x * sigmoid(β*x) con modulación contextual.
"""
function volumetric_swish(
    tensor::Array{T,3}, 
    parameters=nothing
) where T <: AbstractFloat
    # Parámetros por defecto
    if isnothing(parameters)
        beta = 1.0f0
        context_factor = 0.1f0
    else
        beta = isa(parameters, ActivationParameters) ? parameters.beta : 1.0f0
        context_factor = isa(parameters, ActivationParameters) ? parameters.context_factor : 0.1f0
    end
    
    result = similar(tensor)
    
    # Aplicar Swish con modulación contextual basada en valores vecinos
    for x in 2:size(tensor, 1)-1
        for y in 2:size(tensor, 2)-1
            for z in 2:size(tensor, 3)-1
                # Valor base
                value = tensor[x, y, z]
                
                # Calcular modulación contextual basada en vecinos
                neighbors = [
                    tensor[x-1, y, z], tensor[x+1, y, z],
                    tensor[x, y-1, z], tensor[x, y+1, z],
                    tensor[x, y, z-1], tensor[x, y, z+1]
                ]
                
                context_mod = 1.0f0 + context_factor * cos(mean(neighbors))
                
                # Swish con beta adaptado contextualmente
                beta_adjusted = beta * context_mod
                result[x, y, z] = value * (1.0f0 / (1.0f0 + exp(-beta_adjusted * value)))
            end
        end
    end
    
    # Manejar bordes (simplemente aplicar Swish estándar)
    for i in CartesianIndices(tensor)
        x, y, z = Tuple(i)
        if x == 1 || x == size(tensor, 1) || 
           y == 1 || y == size(tensor, 2) || 
           z == 1 || z == size(tensor, 3)
            result[i] = tensor[i] * (1.0f0 / (1.0f0 + exp(-beta * tensor[i])))
        end
    end
    
    return result
end

"""
    tensor_relu(tensor, parameters=nothing)

ReLU tensorial con fugas adaptativas y respuesta sinusoidal.
"""
function tensor_relu(
    tensor::Array{T,3}, 
    parameters=nothing
) where T <: AbstractFloat
    # Parámetros por defecto
    if isnothing(parameters)
        alpha = 0.01f0
        sine_factor = 0.05f0
    else
        alpha = isa(parameters, ActivationParameters) ? parameters.alpha : 0.01f0
        sine_factor = isa(parameters, ActivationParameters) ? parameters.sine_factor : 0.05f0
    end
    
    result = similar(tensor)
    
    for i in CartesianIndices(tensor)
        value = tensor[i]
        
        if value > 0
            # Para valores positivos: x + pequeña modulación sinusoidal
            result[i] = value + sine_factor * sin(value)
        else
            # Para valores negativos: fuga proporcional a alpha
            result[i] = alpha * value
        end
    end
    
    return result
end

"""
    spatial_activation(tensor, parameters=nothing)

Activación que preserva relaciones espaciales y refuerza gradientes espaciales.
"""
function spatial_activation(
    tensor::Array{T,3}, 
    parameters=nothing
) where T <: AbstractFloat
    # Parámetros por defecto
    gradient_enhancement = isnothing(parameters) ? 0.2f0 : 
                          isa(parameters, ActivationParameters) ? parameters.gradient_enhancement : 0.2f0
    
    result = copy(tensor)
    dim_x, dim_y, dim_z = size(tensor)
    
    # Calcular gradientes espaciales
    for x in 2:dim_x-1
        for y in 2:dim_y-1
            for z in 2:dim_z-1
                # Calcular gradientes en cada dirección
                grad_x = (tensor[x+1, y, z] - tensor[x-1, y, z]) / 2
                grad_y = (tensor[x, y+1, z] - tensor[x, y-1, z]) / 2
                grad_z = (tensor[x, y, z+1] - tensor[x, y, z-1]) / 2
                
                # Magnitud del gradiente
                grad_magnitude = sqrt(grad_x^2 + grad_y^2 + grad_z^2)
                
                # Reforzar el valor basado en la magnitud del gradiente
                result[x, y, z] = tanh(tensor[x, y, z] * (1.0f0 + gradient_enhancement * grad_magnitude))
            end
        end
    end
    
    # Aplicar tanh simple en los bordes
    for i in CartesianIndices(tensor)
        x, y, z = Tuple(i)
        if x == 1 || x == dim_x || y == 1 || y == dim_y || z == 1 || z == dim_z
            result[i] = tanh(tensor[i])
        end
    end
    
    return result
end

"""
    contextual_activation(tensor, parameters=nothing)

Activación que modula respuestas basadas en el contexto local.
"""
function contextual_activation(
    tensor::Array{T,3}, 
    parameters=nothing
) where T <: AbstractFloat
    # Parámetros por defecto
    if isnothing(parameters)
        context_radius = 2
        context_weight = 0.3f0
    else
        context_radius = isa(parameters, ActivationParameters) ? parameters.context_radius : 2
        context_weight = isa(parameters, ActivationParameters) ? parameters.context_weight : 0.3f0
    end
    
    result = similar(tensor)
    dim_x, dim_y, dim_z = size(tensor)
    
    for x in 1:dim_x
        for y in 1:dim_y
            for z in 1:dim_z
                # Valor base
                base_value = tensor[x, y, z]
                
                # Acumular contexto local
                context_sum = 0.0f0
                context_count = 0
                
                # Iterar sobre vecindario
                for dx in -context_radius:context_radius
                    for dy in -context_radius:context_radius
                        for dz in -context_radius:context_radius
                            # Omitir el punto central
                            if dx == 0 && dy == 0 && dz == 0
                                continue
                            end
                            
                            # Calcular coordenadas del vecino
                            nx = x + dx
                            ny = y + dy
                            nz = z + dz
                            
                            # Verificar límites
                            if 1 <= nx <= dim_x && 1 <= ny <= dim_y && 1 <= nz <= dim_z
                                # Peso basado en distancia
                                distance = sqrt(dx^2 + dy^2 + dz^2)
                                weight = 1.0f0 / distance
                                
                                # Acumular valor ponderado
                                context_sum += tensor[nx, ny, nz] * weight
                                context_count += weight
                            end
                        end
                    end
                end
                
                # Calcular valor contextual promedio
                context_avg = context_count > 0 ? context_sum / context_count : 0.0f0
                
                # Modular respuesta basada en contraste con contexto
                context_contrast = base_value - context_avg
                result[x, y, z] = tanh(base_value + context_weight * context_contrast)
            end
        end
    end
    
    return result
end

"""
    temporal_activation(tensor, parameters)

Activación que simula dinámica temporal dentro del espacio tensorial.
"""
function temporal_activation(
    tensor::Array{T,3}, 
    parameters
) where T <: AbstractFloat
    # Esta función requiere historial temporal, que debe estar en parameters
    if isnothing(parameters) || !isa(parameters, ActivationParameters) || isempty(parameters.history)
        # Si no hay historial, aplicar tanh simple
        return tanh.(tensor)
    end
    
    # Extraer historial temporal
    history = parameters.history
    
    # Aplicar activación con memoria temporal
    result = similar(tensor)
    
    # Obtener tensor previo más reciente
    prev_tensor = history[end]
    
    # Asegurar dimensiones compatibles
    if size(prev_tensor) != size(tensor)
        prev_tensor = tensor_interpolation(prev_tensor, size(tensor))
    end
    
    for i in CartesianIndices(tensor)
        # Calcular diferencia con valor anterior
        delta = tensor[i] - prev_tensor[i]
        
        # Modular respuesta actual basada en tendencia
        # - Amplificar cambios en la misma dirección (momentum)
        # - Suavizar inversiones bruscas
        momentum_factor = 1.0f0 + 0.2f0 * sign(delta * prev_tensor[i])
        
        result[i] = tanh(tensor[i] * momentum_factor)
    end
    
    return result
end

"""
    feature_activation(tensor, parameters=nothing)

Activación que enfatiza características distintivas en el tensor.
"""
function feature_activation(
    tensor::Array{T,3}, 
    parameters=nothing
) where T <: AbstractFloat
    # Parámetros por defecto
    feature_threshold = isnothing(parameters) ? 0.5f0 : 
                        isa(parameters, ActivationParameters) ? parameters.feature_threshold : 0.5f0
    
    result = similar(tensor)
    
    # Calcular estadísticas globales
    tensor_mean = mean(tensor)
    tensor_std = std(tensor)
    
    # Normalizar tensor
    normalized = (tensor .- tensor_mean) ./ max(tensor_std, 1e-5f0)
    
    for i in CartesianIndices(tensor)
        # Valores normalizados
        norm_value = normalized[i]
        
        # Enfatizar valores que superan el umbral (características)
        if abs(norm_value) > feature_threshold
            # Amplificar características distintivas
            emphasis = 1.0f0 + 0.5f0 * (abs(norm_value) - feature_threshold) / (1.0f0 - feature_threshold)
            result[i] = tanh(tensor[i] * emphasis)
        else
            # Suavizar ruido/características no distintivas
            result[i] = tanh(tensor[i])
        end
    end
    
    return result
end

"""
    general_activation(tensor, parameters=nothing)

Activación general que combina diferentes tipos según el contexto.
"""
function general_activation(
    tensor::Array{T,3}, 
    parameters=nothing
) where T <: AbstractFloat
    # Por defecto, aplicar una combinación de activaciones
    spatial_result = spatial_activation(tensor, parameters)
    contextual_result = contextual_activation(tensor, parameters)
    feature_result = feature_activation(tensor, parameters)
    
    # Pesos para la combinación
    spatial_weight = 0.3f0
    contextual_weight = 0.4f0
    feature_weight = 0.3f0
    
    # Combinar resultados
    result = spatial_weight .* spatial_result .+
             contextual_weight .* contextual_result .+
             feature_weight .* feature_result
    
    return result
end